{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878dbb67",
   "metadata": {},
   "source": [
    "# 18-1. 들어가며\n",
    "\n",
    "\n",
    "## 오늘은 어떤 걸 배우나요?\n",
    "---\n",
    "안녕하세요 여러분.\n",
    "\n",
    "그동안 딥러닝을 이용한 몇 가지 프로젝트를 수행해 보고, 관련하여 딥러닝 프레임워크를 활용해 보면서 딥러닝의 문제 구성이 어떠한지 대략적인 흐름을 익혔을 것입니다.\n",
    "그러나 딥러닝 프레임워크 안쪽에서 실제로 어떤 메커니즘으로 모델이 학습되는지 그 디테일까지 명확하게 파악하지는 못해서 나름의 답답함을 느끼고 계셨을지도 모르겠습니다.\n",
    "\n",
    "오늘은 딥러닝 내부를 좀 더 깊게 들여다보겠습니다. 신경망(Neural Network)이 어떤 식으로 구성되어 있고, 그 과정에서 어떤 용어들이 사용되는지 배웁니다. 그 과정에서 여러분들은 그동안 활용해 왔던 딥러닝 프레임워크를 사용하지 않고, 오직 Numpy만을 이용해 간단한 신경망과 그 훈련 과정을 직접 구현해 볼 것입니다. 이 과정을 통해서 여러분들은 딥러닝 메커니즘에 대해 이전보다는 훨씬 명쾌한 이해를 얻게 되시기라 기대합니다.\n",
    "\n",
    "자 그럼 시작해 보겠습니다.\n",
    "\n",
    "\n",
    "## 학습 목표\n",
    "---\n",
    "- 딥러닝 문제 구성에 대한 기본적인 이해를 높인다.\n",
    "- Neural Network에 사용되는 용어들에 대한 이해를 높인다.\n",
    "- 딥러닝 프레임워크를 사용하지 않고, Numpy만을 이용해 딥러닝 모델과 훈련 과정을 직접 구현해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656547fc",
   "metadata": {},
   "source": [
    "# 18-2. 신경망 구성 (1) 개요\n",
    "\n",
    "**신경망(Neural Network)**이란 무엇일까요?\n",
    "\n",
    "우리 뇌에는 `1000억 개`에 가까운 신경계 뉴런들이 있다고 합니다. 이 뉴런들은 서로 매우 복잡하게 얽혀 있고, 조금 물러서서 보면 하나의 거대한 그물망과 같은 형태를 이루고 있습니다. 보통 우리는 이를 신경망이라고 부릅니다.\n",
    "\n",
    "예전부터 인류는 자연의 모습을 본떠 인공적인 물건을 만들려는 시도를 많이 해왔습니다. 물론 이것이 자연의 모습을 본뜬 것만이 성공적이라는 뜻은 아니지만요. (새가 나는 방식과 비행기가 나는 방식은 다른 것처럼요.)\n",
    "\n",
    "머신러닝/딥러닝 과학자들도 자연에서 답을 찾으려 노력했고, 우리 뇌 속의 신경망 구조에 착안해서 **퍼셉트론(Perceptron)**이라는 형태를 제안하며 이를 연결한 형태를 인공신경망(Artificial Neural Network)이라고 부르기 시작했습니다.\n",
    "\n",
    "\n",
    "## MNIST Revisited\n",
    "---\n",
    "아마도 여러분은 AIFFEL 학습 과정 중에 딥러닝 모델 학습을 몇 번 경험해 보셨을 것입니다. 그렇다면 아마도 가장 처음 다루어본 데이터셋은 틀림없이 MNIST라는 숫자 이미지 데이터셋이었을 것입니다. 딥러닝 프레임워크를 이용하면 몇 줄 안되는 코드만으로 MNIST 데이터셋을 99% 이상의 정확도로 분류할 수 있는 이미지 분류기를 만들 수 있으며, 이를 활용해서 다양한 카테고리의 이미지 분류기로 확장해 나갈 수 있다는 것도 이미 알고 계실 것입니다.\n",
    "\n",
    "그럼 한번 MNIST 이미지 분류기 모델이 어떻게 구성되었는지 기억을 되짚어 볼까요? 아래는 이미 여러 번 구현해 보았던 Tensorflow 기반 분류 모델 예시 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8779bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 10s 1ms/step - loss: 0.4871 - accuracy: 0.8835\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2303 - accuracy: 0.9344\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1812 - accuracy: 0.9478\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1506 - accuracy: 0.9575\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1295 - accuracy: 0.9635\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1140 - accuracy: 0.9674\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1011 - accuracy: 0.9714\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0912 - accuracy: 0.9738\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0825 - accuracy: 0.9763\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0757 - accuracy: 0.9790\n",
      "313/313 - 1s - loss: 0.1101 - accuracy: 0.9658\n",
      "test_loss: 0.11008495092391968 \n",
      "test_accuracy: 0.9657999873161316\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230a8bc",
   "metadata": {},
   "source": [
    "이미 여러분은 Conv2D같이 이미지 분류에 특화된 모델을 사용해 본 경험이 있을 것입니다만, 오늘은 그보다 가장 기본적인 신경망 형태인 다층 퍼셉트론(Multi-Layer Perceptron; MLP)만을 이용해서 더욱 간단하게 구현해 보았습니다. 뇌 속의 뉴런이 1000억 개라지만 위에서 만든 모델은 굳이 Conv2D를 사용하지도 않았는데도 39,760개의 파라미터만으로 테스트 성능이 97%에 육박하고 있습니다.\n",
    "오늘은 인공신경망의 실제 구현 원리를 보다 명확하게 이해하기 위해, 그동안 들춰보지 않았던 프레임워크 내부에서 일어나는 일을 Numpy를 활용해 직접 구현해 보면서 이해해 보고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee47493",
   "metadata": {},
   "source": [
    "## 다층 퍼셉트론 Overview\n",
    "---\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-1.max-800x600.png)\n",
    "\n",
    "위의 이미지는 총 3개의 레이어로 구성된 퍼셉트론을 나타냅니다. 위에서 보았던 예시 코드와도 동일합니다. 은닉층에는 `H개의 노드`가, 출력층에는 `K개의 노드`가 존재하는 인공신경망을 표현한 것입니다. (+1 부분은 bias를 뜻하는 부분이므로 이전 레이어와의 연결이 없습니다. ) 위의 코드에서는 `H=50`, `K=10`, 그리고 입력층 노드 개수 `d=784`로 정의되었습니다.\n",
    "\n",
    "bias 노드에 대해서는 아래 참고 자료를 확인해 주세요.\n",
    "\n",
    "**참고자료 - bias term**\n",
    "\n",
    "- [What is the role of the bias in neural networks?](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)\n",
    "\n",
    "이제 각 레이어에 대해 한번 알아보겠습니다.\n",
    "\n",
    "위의 이미지를 보면 입력값이 있는 입력층(input layer), 최종 출력값이 있는 출력층(output layer), 그리고 그 사이에 있는 층인 은닉층(hidden layer)이 있습니다. 보통 입력층과 출력층 사이에 몇 개의 층이 존재하든 모두 은닉층이라고 부릅니다.\n",
    "\n",
    "보통 그림으로 인공신경망을 표현할 때에는 노드를 기준으로 레이어를 표시해서 3개의 레이어라고 생각할 수 있지만, 실제로는 `총 2개의 레이어`를 가졌습니다. 레이어 개수를 셀 때는 노드와 노드 사이의 연결하는 부분이 몇 개 존재하는지 세면 보다 쉽게 알 수 있습니다.\n",
    "\n",
    "이렇게 인공신경망이 어떻게 생겼는지 대략 알아봤는데요. 인공신경망 중에서도 위의 이미지처럼 2개 이상의 레이어를 쌓아서 만든 것을 보통 `다층 퍼셉트론(Multi-Layer Perceptron; MLP)`이라고 부릅니다. 그리고 입력층, 출력층을 제외한 은닉층이 많아지면 많아질수록 인공신경망이 `DEEP` 해졌다고 이야기합니다.\n",
    "\n",
    "여기서 좀 감이 오시나요? 우리가 지금 알아보려고 하는 딥러닝이 바로 이 인공신경망이 `DEEP`해졌다는 뜻에서 나온 단어입니다. 그래서 우리가 하려는 딥러닝은 충분히 깊은 인공신경망을 활용하며 이를 보통 다른 단어로 `DNN(Deep Neural Network)`라고 부릅니다.\n",
    "\n",
    "> 💡 Tips\n",
    "Fully-Connected Neural Network와 같은 단어를 들어보신 분들이 있으신가요? 이는 앞에서 설명드렸던 MLP의 다른 용어입니다. 이 Fully-Connnected Nerual Network는 서로 다른 층에 위치한 노드 간에는 연결 관계가 존재하지 않으며, 인접한 층에 위치한 노드들 간의 연결만 존재한다는 의미를 내포합니다.\n",
    "\n",
    "\n",
    "## Parameters/Weights\n",
    "---\n",
    "앞에서 설명한 입력층-은닉층, 은닉층-출력층 사이에는 사실 각각 행렬(Matrix)이 존재합니다. 여러분 모두 고등학교 때 배우셨던 행렬 곱셈 기억나시겠죠?\n",
    "\n",
    "예를 들어 입력값이 100개, **은닉 노드가 20개**라면 사실 이 입력층-은닉층 사이에는 `100x20`의 형태를 가진 행렬이 존재합니다. 똑같이, MNIST 데이터처럼 10개의 클래스를 맞추는 문제를 풀기 위해 **출력층이 10개의 노드**를 가진다면 은닉층-출력층 사이에는 `20x10`의 형태를 가진 행렬이 존재하게 됩니다.\n",
    "\n",
    "이 행렬들을 Parameter 혹은 Weight라고 부릅니다. 두 단어는 보통 같은 뜻으로 사용되지만, 실제로 Parameter에는 위의 참고 자료에서 다룬 bias 노드도 포함된다는 점만 유의해 주세요.\n",
    "\n",
    "이때 인접한 레이어 사이에는 아래와 같은 관계가 성립합니다.\n",
    "\n",
    "$$y = {W}\\cdot{X} + b$$\n",
    "\n",
    "우리가 위에서 간단히 만들어 보았던 MLP 기반 딥러닝 모델을 Numpy로 다시 만들어 본다고 생각해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267c4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6b1b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dc8cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.74066368, -0.84803055, -1.06702387, -0.27320309, -1.50001066,\n",
       "        0.35702953, -0.19952357, -1.49686694,  0.68214693, -0.5418326 ,\n",
       "       -0.01747973,  0.93264989,  0.62658048,  0.05660688,  0.71287527,\n",
       "       -0.02937456, -0.61315357,  1.01211912, -0.99316559, -0.60372005,\n",
       "        1.19676974,  0.65942336, -0.00913503, -0.21842772, -1.33700664,\n",
       "        1.43188489,  1.06266228,  0.19653666, -0.45974342, -0.05535014,\n",
       "        1.54019945,  0.2416061 , -1.09800779, -0.00419456, -0.02676311,\n",
       "        0.09210111,  0.82391295,  0.0265218 , -0.51148094, -0.92176452,\n",
       "       -1.05142546, -0.04834452, -0.89683247,  0.28103258, -0.31163045,\n",
       "        0.29445327, -1.4303658 , -0.17967109,  0.61628424,  2.12971437])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04691c",
   "metadata": {},
   "source": [
    "# 18-3. 신경망 구성 (2) 활성화 함수와 손실 함수\n",
    "\n",
    "\n",
    "## 활성화 함수 (Activation Functions)\n",
    "---\n",
    "MLP의 또 다른 중요한 구성요소는 바로 활성화 함수인데요. 딥러닝에서는 이 활성화 함수의 존재가 필수적입니다. 수학적인 이유가 있지만, 간단히만 설명하자면 이 활성화 함수는 보통 비선형 함수를 사용하는데 이 비선형 함수를 MLP 안에 포함시키면서 모델의 표현력이 좋아지게 됩니다. (정확히는 레이어 사이에 이 비선형 함수가 포함되지 않은 MLP는 한 개의 레이어로 이루어진 모델과 수학적으로 다른 점이 없습니다.)\n",
    "\n",
    "- [Why must a nonlinear activation function be used in a backpropagation neural network?](https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net/54503251#54503251)\n",
    "\n",
    "그럼 이렇게 중요한 역할을 하는 활성화 함수에는 어떤 종류가 있을까요? 몇 가지 자주 쓰이는 활성화 함수들에 대해서 알아보도록 하겠습니다.\n",
    "\n",
    "**1. Sigmoid**\n",
    "$$σ(x)=\\frac{1}{1+e^{−x}}$$\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-2.max-800x600.png)\n",
    "<center>[출처 : https://reniew.github.io/12/]</center>\n",
    "\n",
    "이전 스텝에서 우리는 은닉층의 출력에다 활성화 함수로 sigmoid를 사용한 바 있습니다.\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))\n",
    "```\n",
    "\n",
    "그럼 첫 번째 은닉층의 출력 a1에다가 sigmoid를 적용해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d20367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85077135 0.29984616 0.25596947 0.43212091 0.18242393 0.58832118\n",
      " 0.45028393 0.18289327 0.6642177  0.36776138 0.49563018 0.71761258\n",
      " 0.65171369 0.51414794 0.67103618 0.49265689 0.35134016 0.73343466\n",
      " 0.27028727 0.35349307 0.76794964 0.65913084 0.49771626 0.44560915\n",
      " 0.20800275 0.80719483 0.74319898 0.54897662 0.38704669 0.486166\n",
      " 0.82349372 0.56010941 0.25011336 0.49895136 0.49330962 0.52300901\n",
      " 0.69506632 0.50663006 0.37484642 0.2845985  0.25895147 0.48791622\n",
      " 0.28970186 0.56979936 0.42271681 0.57308602 0.1930417  0.45520267\n",
      " 0.64937299 0.89375789]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 element가 0에서 1사이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7849652",
   "metadata": {},
   "source": [
    "예전부터 활성화 함수로 많이 써오던 sigmoid 함수입니다. 현재는 아래에서 설명할 ReLU 함수를 더 많이 사용하는데 그 이유는 다음과 같습니다.\n",
    "\n",
    "- **vanishing gradient** 현상이 발생한다.\n",
    "- exp 함수 사용 시 비용이 크다.\n",
    "\n",
    "아래 참고 자료는 아래 섹션인 오차역전파법(BackPropagation)을 공부하신 다음 보시면 더욱 이해하기 편할 수 있습니다.\n",
    "\n",
    "- [Vanishing Gradient Problem](https://brunch.co.kr/@chris-song/39)\n",
    "\n",
    "\n",
    "**2.Tanh**\n",
    "$$tanh(x)=\\frac{e^x−e^{−x}}{e^x+e^{−x}}$$\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-3.max-800x600.png)\n",
    "<center>[출처 : https://reniew.github.io/12/]</center>\n",
    "\n",
    "- tanh 함수는 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결.\n",
    "- **vanishing gradient** 문제 존재.\n",
    "\n",
    "\n",
    "**3.ReLU**\n",
    "$$f(x)=max(0,x)$$\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-4.max-800x600.png)\n",
    "<center>[출처 : https://reniew.github.io/12/]</center>\n",
    "\n",
    "- sigmoid, tanh 함수에 비해 학습이 빠름.\n",
    "- 연산 비용이 크지 않고, 구현이 매우 간단하다.\n",
    "\n",
    "활성화 함수에 대한 참고 자료들로 아래 링크의 페이지들을 추천합니다.\n",
    "\n",
    "**참고 자료**\n",
    "\n",
    "- [딥러닝에서 사용하는 활성화 함수](https://reniew.github.io/12/)\n",
    "- [Activation Function](https://pozalabs.github.io/Activation_Function/)\n",
    "- [위키독스: 비선형 활성화 함수](https://wikidocs.net/60683)\n",
    "\n",
    "sigmoid 다음에 다시 Dense 레이어가 출현합니다. 출력 노드 개수만 다를 뿐 동일한 구조입니다. 그렇다면 MLP 레이어를 아래와 같이 함수로 구현할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbf8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f75519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09821866 -0.03435659 -0.57476225  0.11628983  0.10748678  0.31783601\n",
      " -0.72520056  0.51604276  0.14050023 -0.06241824]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56736f50",
   "metadata": {},
   "source": [
    "우리 모델의 최종 출력인 `a2`에 softmax 함수를 적용해 봅시다. 그러면 모델의 출력은 입력 `X`가 10가지 숫자 중 하나일 확률의 형태로 멋지게 가공됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1397ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e4dbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08803555, 0.09384109, 0.05466355, 0.10909829, 0.10814211,\n",
       "       0.13345915, 0.04702874, 0.16271532, 0.11177184, 0.09124436])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1db5d8",
   "metadata": {},
   "source": [
    "## 손실함수 (Loss Functions)\n",
    "---\n",
    "이렇게 비선형 활성화 함수를 가진 여러 개의 은닉층을 거친 다음 신호 정보들은 출력층으로 전달됩니다. 이때 우리가 원하는 정답과 전달된 신호 정보들 사이의 차이를 계산하고, 이 차이를 줄이기 위해 각 파라미터들을 조정하는 것이 딥러닝의 전체적인 학습 흐름입니다.\n",
    "\n",
    "이 차이를 구하는 데 사용되는 함수는 손실함수(Loss function) 또는 비용함수(Cost function)라고 부릅니다. 대표적으로 다음과 같은 두 가지 손실함수가 존재합니다.\n",
    "\n",
    "**평균제곱오차 (MSE: Mean Square Error)**\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(Y_i−{\\hat Y_i})2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c5e6d",
   "metadata": {},
   "source": [
    "**교차 엔트로피 (Cross Entropy)**\n",
    "\n",
    "오늘 우리는 바로 Cross Entropy를 이용해 모델을 학습할 것입니다. Cross Entropy는 두 확률분포 사이의 유사도가 클수록 작아지는 값입니다. 아직 별로 학습되지 않은 현재의 모델이 출력하는 softmax 값 $\\hat{y}$은 10개의 숫자 각각의 확률이 대부분 0.1 근처를 오가는 정도입니다.\n",
    "\n",
    "모델을 학습하게 되면, $\\hat{y}$이 점점 정답에 가까워지게 됩니다. 정말 그렇게 되는지 다음 스텝에서 살펴봅시다. 우선은 $\\hat{y}$과 정답을 비교해 봅시다.\n",
    "$$E=−\\sum_{i=1}^{n}t_ilogy_i$$\n",
    "\n",
    "**참고 자료**\n",
    "\n",
    "- [Understanding different Loss Functions for Neural Networks.](https://shiva-verma.medium.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718)\n",
    "- [손실함수(Loss Function)](http://www.gisdeveloper.co.kr/?p=7631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90423987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_one_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb99de",
   "metadata": {},
   "source": [
    "우리는 모델의 최종 출력인 `softmax(a2)`와 정답 라벨의 One-hot 인코딩의 분포가 유사해지기를 기대합니다. 아직은 별로 유사하지 않은 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a1f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08803555 0.09384109 0.05466355 0.10909829 0.10814211 0.13345915\n",
      " 0.04702874 0.16271532 0.11177184 0.09124436]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4c9bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2938727479398247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea4276",
   "metadata": {},
   "source": [
    "# 18-4. 경사하강법\n",
    "\n",
    "그럼 이제 오차는 구했습니다. 다음 단계는 무엇일까요?\n",
    "\n",
    "앞서 말씀드렸지만 우리는 이 오차를 줄이는 것이 목표입니다. 이 상황은 우리가 마치 산꼭대기에 서 있는 것과 동일하게 생각할 수 있습니다.\n",
    "\n",
    "우리는 얼른 이 산에서 내려가서 집에 가고 싶은 사람들이죠. 여러분들이라면 어디로 내려가야 할지 모를 때 어떻게 하시겠어요?\n",
    "\n",
    "아마도 내리막길을 따라 일단 내려가 보지 않을까요?\n",
    "\n",
    "**경사하강법(Gradient Descent)** 또한 동일한 원리입니다. 각 단계에서의 기울기를 구해서 해당 기울기가 가리키는 방향으로 이동하는 방법이죠.\n",
    "\n",
    "**참고 자료**\n",
    "\n",
    "- [경사하강법](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14v3-3-1.max-800x600.png)\n",
    "[https://bioinformaticsandme.tistory.com/134](https://bioinformaticsandme.tistory.com/134)\n",
    "\n",
    "위의 이미지처럼 각 시점의 기울기가 가리키는 방향으로 이동해나가는 것이죠. 그럼 경사하강법을 사용하면 항상 산 아래에 잘 도착할 수 있을까요?\n",
    "\n",
    "예를 들어, 우리가 너무 크게 발걸음을 내디딜 수 있는 거인이라면 아마도 산 아래로 내려가지 못하고 또 다른 골짜기에 빠지고 말 것입니다. 그래서 우리는 **학습률(learning rate)** 이라는 개념을 도입해 기울기 값과 이 학습률을 곱한 만큼만 발걸음을 내딛습니다.\n",
    "\n",
    "**참고 자료**\n",
    "\n",
    "- [[머신러닝] lec 7-1 : 학습 Learning rate, Overfitting, 그리고 일반화](https://aileen93.tistory.com/71)\n",
    "\n",
    "또한, 이미 생각해 보신 분들도 있겠지만, 아무리 우리가 발걸음을 잘 내디딘다고 해도 어디서 출발했느냐에 따라 산 아래로 내려가는 시간이 빨라질 수도 느려질 수도 있습니다. 이는 **parameter의 값들을 어떻게 초기화하는지**의 문제와 맞닿아 있으니, 아래의 참고 자료를 확인해 주세요.\n",
    "\n",
    "**참고 자료**\n",
    "\n",
    "- [가중치 초기화 (Weight Initialization)](https://reniew.github.io/13/)\n",
    "\n",
    "X-Y 좌표축의 기울기란 X의 변화에 따른 Y의 변화량을 의미합니다. Y를 X로 미분해서 구하지요.\n",
    "\n",
    "우리는 파라미터 W의 변화에 따른 오차(Loss) L의 변화량을 구하려고 합니다. 그러면 오차 기울기가 커지는 방향의 반대 방향으로 파라미터를 조정해 주면 됩니다. 단, 조정을 너무 많이 해주면 안 되기 때문에 적절한 step size 역할을 하는 learning rate가 필수적입니다. 그 과정을 Numpy를 통해 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5c8134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01760711,  0.01876822,  0.01093271,  0.02181966,  0.02162842,\n",
       "        -0.17330817,  0.00940575,  0.03254306,  0.02235437,  0.01824887],\n",
       "       [-0.17916019,  0.01624   ,  0.01211142,  0.02387047,  0.02042529,\n",
       "         0.02724337,  0.01015278,  0.03044802,  0.02162344,  0.01704541],\n",
       "       [ 0.02238304,  0.01519522,  0.00949896,  0.02630658, -0.17884639,\n",
       "         0.02086798,  0.0157213 ,  0.02916214,  0.02178794,  0.01792322],\n",
       "       [ 0.0169432 , -0.18548751,  0.01113605,  0.02660016,  0.02317333,\n",
       "         0.02320377,  0.01230622,  0.02715011,  0.02630824,  0.01866643],\n",
       "       [ 0.01854087,  0.01775194,  0.01025628,  0.02457405,  0.02153259,\n",
       "         0.01966094,  0.0116049 ,  0.0338551 ,  0.02264977, -0.18042643]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a1bf5",
   "metadata": {},
   "source": [
    "![](img/6.PNG)\n",
    "[Classification and Loss Evaluation - Softmax and Cross Entropy Loss](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d55d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09579775, -0.0792451 ,  0.0254553 ,  0.05538447,  0.01326968,\n",
       "        -0.06328549,  0.02528319,  0.06927116,  0.0527343 , -0.00306976],\n",
       "       [-0.05007668, -0.02901694,  0.0321286 ,  0.07435716, -0.11079938,\n",
       "        -0.06134029,  0.03670404,  0.09277788,  0.06835632, -0.0530907 ],\n",
       "       [-0.04572658, -0.04626585,  0.0242222 ,  0.0558628 , -0.07489349,\n",
       "        -0.03883445,  0.02739442,  0.06868221,  0.05157855, -0.02201981],\n",
       "       [-0.06643805, -0.08302849,  0.02782297,  0.06324476, -0.01808882,\n",
       "        -0.01788587,  0.0299229 ,  0.07816083,  0.05916512, -0.07287535],\n",
       "       [-0.04362844, -0.05413581,  0.01868738,  0.04322201, -0.0636393 ,\n",
       "        -0.01876756,  0.02126868,  0.0518719 ,  0.03985558,  0.00526554],\n",
       "       [-0.00276416, -0.05049482,  0.0273297 ,  0.06311794, -0.0459856 ,\n",
       "        -0.06867179,  0.0305009 ,  0.07986841,  0.05910104, -0.09200162],\n",
       "       [-0.08181109, -0.05841812,  0.03804526,  0.08654171, -0.06891766,\n",
       "        -0.06629073,  0.04159508,  0.10857573,  0.0804257 , -0.07974588],\n",
       "       [-0.07708506, -0.12028322,  0.03575809,  0.08086034, -0.02053357,\n",
       "        -0.04952962,  0.03812304,  0.09977354,  0.07612615, -0.06320968],\n",
       "       [-0.07053273, -0.11252889,  0.02313164,  0.05143353,  0.01657399,\n",
       "        -0.02385088,  0.02363082,  0.0624835 ,  0.04898827, -0.01932926],\n",
       "       [-0.08718088, -0.06967764,  0.03177597,  0.07080823, -0.00129326,\n",
       "        -0.05631451,  0.03298014,  0.08951128,  0.06667545, -0.07728477],\n",
       "       [-0.03933391, -0.05381078,  0.03954812,  0.09114286, -0.08711939,\n",
       "        -0.08187109,  0.04427423,  0.11488744,  0.08466234, -0.1123798 ],\n",
       "       [-0.05231432, -0.0296185 ,  0.02075386,  0.04624937, -0.0139311 ,\n",
       "        -0.05217465,  0.02173114,  0.05898292,  0.04343399, -0.04311271],\n",
       "       [-0.06677545, -0.02493942,  0.0231652 ,  0.05086369, -0.00736838,\n",
       "        -0.07619419,  0.02363883,  0.06560723,  0.04800525, -0.03600276],\n",
       "       [-0.07657606, -0.01775304,  0.02375083,  0.05363355, -0.06319357,\n",
       "        -0.04412665,  0.02596766,  0.06717234,  0.04942399, -0.01829904],\n",
       "       [-0.02354152, -0.04747193,  0.02731477,  0.06369267, -0.08830954,\n",
       "        -0.04871931,  0.03143658,  0.07883461,  0.05881834, -0.05205468],\n",
       "       [-0.10843479, -0.04525383,  0.0430683 ,  0.09791324, -0.07633769,\n",
       "        -0.05765973,  0.04704447,  0.12347154,  0.09060223, -0.11441375],\n",
       "       [-0.02821956, -0.02887819,  0.01367714,  0.03046391, -0.00900847,\n",
       "        -0.04225905,  0.01430088,  0.03859037,  0.0287964 , -0.01746342],\n",
       "       [-0.04826878, -0.0490461 ,  0.02723359,  0.06387987, -0.07681044,\n",
       "         0.00614556,  0.03144045,  0.07847189,  0.05846867, -0.09151471],\n",
       "       [-0.09581247, -0.02426059,  0.02576204,  0.05794128, -0.02291203,\n",
       "        -0.00876068,  0.02740815,  0.07328238,  0.05352479, -0.08617287],\n",
       "       [-0.00275119, -0.10275498,  0.02745622,  0.06509177, -0.08228011,\n",
       "        -0.02048622,  0.03219069,  0.0782421 ,  0.0603783 , -0.05508659],\n",
       "       [-0.04423683, -0.05337881,  0.03168441,  0.07222473, -0.04848878,\n",
       "        -0.06994927,  0.03463652,  0.0911605 ,  0.06747964, -0.0811321 ],\n",
       "       [-0.02422436, -0.03272563,  0.02084279,  0.04939414, -0.08928195,\n",
       "        -0.01081998,  0.02481047,  0.06022458,  0.04499551, -0.04321556],\n",
       "       [-0.0717379 , -0.07224205,  0.03672555,  0.08362074, -0.06683439,\n",
       "        -0.06902327,  0.04019652,  0.10431783,  0.07791744, -0.06294047],\n",
       "       [-0.04649184, -0.02305013,  0.01734726,  0.03768785,  0.00752567,\n",
       "        -0.06930797,  0.01725747,  0.04903336,  0.03592622, -0.02592789],\n",
       "       [-0.06601411, -0.04466666,  0.0278813 ,  0.06216611, -0.01091607,\n",
       "        -0.06772162,  0.02911319,  0.07934789,  0.05849959, -0.06768962],\n",
       "       [ 0.01046318, -0.07194065,  0.01644609,  0.04018466, -0.09914808,\n",
       "        -0.0106408 ,  0.02073126,  0.04636709,  0.03674612,  0.01079113],\n",
       "       [-0.00690963, -0.04567162,  0.02353595,  0.05662965, -0.08743242,\n",
       "         0.01192279,  0.02842562,  0.06913863,  0.05158874, -0.10122771],\n",
       "       [-0.04118302, -0.06631994,  0.02365085,  0.05328704, -0.03483491,\n",
       "        -0.06925467,  0.02537598,  0.06615761,  0.05023183, -0.00711077],\n",
       "       [-0.10706672, -0.01301136,  0.0231605 ,  0.05123573, -0.04114172,\n",
       "        -0.0369001 ,  0.02432205,  0.06455787,  0.04730652, -0.01246277],\n",
       "       [-0.08702777, -0.08884861,  0.03314885,  0.07589152, -0.05495545,\n",
       "        -0.01308366,  0.03645612,  0.09296193,  0.07037997, -0.06492291],\n",
       "       [-0.05365545, -0.09429654,  0.03272801,  0.07567681, -0.08396345,\n",
       "        -0.03997754,  0.03689409,  0.09213419,  0.0702344 , -0.03577454],\n",
       "       [-0.11232295, -0.08175435,  0.03525606,  0.07767559, -0.00979447,\n",
       "        -0.08999559,  0.03608184,  0.09779892,  0.07339247, -0.02633752],\n",
       "       [-0.04871031, -0.03002616,  0.02439507,  0.05435457, -0.01174783,\n",
       "        -0.07343517,  0.02549023,  0.06998915,  0.05122109, -0.06153065],\n",
       "       [-0.09653814, -0.05133907,  0.03731964,  0.08323897, -0.04049592,\n",
       "        -0.09928762,  0.03932644,  0.10575332,  0.07802326, -0.05600087],\n",
       "       [-0.01325434, -0.10645422,  0.01912675,  0.04499439, -0.04101962,\n",
       "        -0.00390178,  0.02193015,  0.05274483,  0.04211061, -0.01627678],\n",
       "       [-0.0934802 , -0.11719067,  0.03586183,  0.08120517, -0.01763395,\n",
       "        -0.02338077,  0.03826644,  0.09997193,  0.07612331, -0.07974309],\n",
       "       [-0.06112456, -0.03171804,  0.02708419,  0.06074312, -0.01485174,\n",
       "        -0.05679732,  0.028592  ,  0.07786612,  0.05689765, -0.08669143],\n",
       "       [-0.10637941, -0.0177486 ,  0.03284515,  0.07370717, -0.03902838,\n",
       "        -0.04475881,  0.03498135,  0.09406211,  0.06826674, -0.09594734],\n",
       "       [-0.02363085, -0.09269566,  0.02464909,  0.05709263, -0.0496089 ,\n",
       "        -0.03518162,  0.02766064,  0.06924087,  0.05343158, -0.03095778],\n",
       "       [-0.0307338 , -0.0603543 ,  0.0325721 ,  0.07437812, -0.03682068,\n",
       "        -0.07368478,  0.03552904,  0.09427156,  0.06974247, -0.10489974],\n",
       "       [-0.03509413, -0.07970875,  0.0311816 ,  0.07135158, -0.04338863,\n",
       "        -0.06310153,  0.03418969,  0.0889993 ,  0.06690157, -0.07133069],\n",
       "       [-0.09240898, -0.04396171,  0.0305301 ,  0.06965758, -0.07594207,\n",
       "        -0.02766061,  0.03377726,  0.08622124,  0.06411451, -0.04432733],\n",
       "       [-0.0607304 , -0.04599553,  0.02822056,  0.06494484, -0.06888612,\n",
       "        -0.02886109,  0.03159082,  0.080553  ,  0.05991785, -0.06075393],\n",
       "       [-0.05754203, -0.04572637,  0.02455844,  0.05490085, -0.0045042 ,\n",
       "        -0.05002473,  0.02566848,  0.06985532,  0.05166858, -0.06885432],\n",
       "       [-0.09643231, -0.08206656,  0.03274608,  0.07226075,  0.01301682,\n",
       "        -0.0710529 ,  0.03330402,  0.09140851,  0.06846654, -0.06165095],\n",
       "       [ 0.00827403, -0.07629916,  0.01971441,  0.04780884, -0.07856906,\n",
       "         0.00640526,  0.02409631,  0.05667297,  0.04389311, -0.05199671],\n",
       "       [-0.07739455, -0.08124183,  0.03024073,  0.06854117, -0.04110163,\n",
       "        -0.03848119,  0.03267083,  0.08453299,  0.06399235, -0.04175887],\n",
       "       [-0.11237059, -0.06620109,  0.03781835,  0.08489006, -0.04859817,\n",
       "        -0.06484414,  0.04029215,  0.106373  ,  0.07915645, -0.05651602],\n",
       "       [ 0.00716775, -0.0495704 ,  0.01533853,  0.03498905, -0.01700812,\n",
       "        -0.06151788,  0.01668832,  0.04394812,  0.03334673, -0.02338209],\n",
       "       [-0.09800186, -0.02174353,  0.03024508,  0.0667629 , -0.02486568,\n",
       "        -0.08242104,  0.0313077 ,  0.08577296,  0.06250396, -0.04956049]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a49d2",
   "metadata": {},
   "source": [
    "같은 방식으로 우리가 학습해야 할 모든 파라미터 `W1`, `b1`, `W2`, `b2`에 대한 기울기를 모두 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c5f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334050a7",
   "metadata": {},
   "source": [
    "중간에 sigmoid가 한번 사용되었으므로, 활성화함수에 대한 gradient도 고려되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba0b210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e151135",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101e28f",
   "metadata": {},
   "source": [
    "파라미터를 업데이트하는 함수를 생각해 봅시다. learning_rate도 고려해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76cea091",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb3771",
   "metadata": {},
   "source": [
    "# 18-5. 오차역전파법이란?\n",
    "\n",
    "이제 우리는 손실 함수를 통해 구해진 오차를 가지고 각 파라미터들을 조정하는 경사하강법에 대해 알게 되었습니다. 그럼 이 기울기를 어떻게 입력층까지 전달하며 파라미터들을 조정해 나갈 수 있을까요? 이 과정에서 쓰이는 개념이 **오차역전파법(Backpropagation)** 입니다.\n",
    "\n",
    "오차역전파법은 앞에서 설명한 MLP를 학습시키기 위한 일반적인 알고리즘 중 하나입니다. 이는 출력층의 결과와 내가 뽑고자 하는 target 값과의 차이를 구한 뒤, 그 오차 값을 각 레이어들을 지나며 역전파 해가며 각 노드가 가지고 있는 변수들을 갱신해 나가는 방식입니다.\n",
    "\n",
    "![](https://d3s0tskafalll9.cloudfront.net/media/images/f-14-6.max-800x600.png)\n",
    "\n",
    "위의 과정을 하나의 레이어에 대해 다음과 같이 정리해 볼 수 있습니다. 이전의 `affine_layer_forward(X, W, b)`에 대응하여 생각해 보면 해당 레이어의 backpropagation 함수를 얻을 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f9ead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d6c6d",
   "metadata": {},
   "source": [
    "이상 정리된 내용을 바탕으로 Forward Propagation과 Backward Propagation이 이루어지는 한 사이클을 아래와 같이 엮어 볼 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5658846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06018596 0.10026647 0.19814437 0.05439017 0.10006914 0.06171153\n",
      "  0.05583797 0.07790676 0.18037149 0.11111615]\n",
      " [0.06787697 0.12228346 0.18462723 0.06511389 0.07621557 0.0725951\n",
      "  0.0536311  0.09462286 0.17017267 0.09286115]\n",
      " [0.07676343 0.15120435 0.15394491 0.0685721  0.06470216 0.08080654\n",
      "  0.05399493 0.07365589 0.18992309 0.08643259]\n",
      " [0.06656209 0.1584627  0.18165938 0.06091368 0.06450945 0.07022895\n",
      "  0.04544381 0.08102029 0.17832055 0.09287911]\n",
      " [0.06346774 0.11761643 0.16960473 0.06721406 0.07318612 0.07110248\n",
      "  0.0515191  0.08380467 0.20993981 0.09254487]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.487120282298341\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_one_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1c844",
   "metadata": {},
   "source": [
    "# 18-6. 모델 학습 Step-by-Step\n",
    "\n",
    "방금 우리는 한 스텝의 Forward Propagation과 Backward Propagation을 통해 학습해야 할 파라미터 `W1`, `b1`, `W2`, `b2`가 업데이트되는 과정을 확인했습니다.\n",
    "\n",
    "과연 이렇게 파라미터가 업데이트될 때, 우리의 모델은 점점 더 정확한 추론을 하게 되는 것일까요? 업데이트되는 과정을 다섯 스텝만 반복해 보면서 그 효과를 확인해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc5ce6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_one_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebe44293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.07709434 0.06787813 0.05658965 0.21268209 0.13635241 0.09396489\n",
      "  0.13267911 0.0812969  0.06050128 0.0809612 ]\n",
      " [0.087868   0.05650809 0.07069932 0.20552769 0.1308247  0.09621131\n",
      "  0.10058833 0.09138486 0.06304742 0.09734027]\n",
      " [0.07686235 0.08340882 0.05860818 0.21299966 0.15263886 0.08487603\n",
      "  0.10314279 0.07978482 0.07738299 0.07029551]\n",
      " [0.08753394 0.0877991  0.07426636 0.18410134 0.1414599  0.09511073\n",
      "  0.10465027 0.0756737  0.06843592 0.08096875]\n",
      " [0.07493294 0.07021287 0.07337605 0.22045536 0.1387067  0.08498244\n",
      "  0.10665707 0.06578675 0.06832632 0.0965635 ]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.2893385360832923\n",
      "---------\n",
      "[[0.09663699 0.08260085 0.05297436 0.15630202 0.14797797 0.11970005\n",
      "  0.11445945 0.0742061  0.05541079 0.09973142]\n",
      " [0.11392379 0.06733214 0.06549676 0.15099676 0.14000832 0.11736624\n",
      "  0.08605761 0.0819762  0.05757652 0.11926568]\n",
      " [0.09208283 0.0993922  0.05420286 0.16239898 0.17230587 0.10034264\n",
      "  0.08997302 0.07242477 0.07172286 0.08515398]\n",
      " [0.10323805 0.10732295 0.06847822 0.1388631  0.15217852 0.11212469\n",
      "  0.09084154 0.06856002 0.0622207  0.09617222]\n",
      " [0.09255289 0.08478736 0.06812426 0.16130884 0.15194467 0.1027367\n",
      "  0.09228609 0.05940788 0.06272684 0.12412448]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.0743718236550124\n",
      "---------\n",
      "[[0.11331081 0.09467928 0.04815213 0.12100033 0.15221331 0.14252532\n",
      "  0.09775796 0.06616961 0.04945332 0.11473792]\n",
      " [0.13836559 0.07573103 0.05902651 0.11708069 0.14240493 0.13418827\n",
      "  0.07306002 0.07205446 0.05136282 0.13672569]\n",
      " [0.10440956 0.11281488 0.04902923 0.12997532 0.18620541 0.11226707\n",
      "  0.0780031  0.06462    0.06519472 0.09748071]\n",
      " [0.11562286 0.12535883 0.06198335 0.11031022 0.15728129 0.12553497\n",
      "  0.07866263 0.06125676 0.05566735 0.10832173]\n",
      " [0.10699329 0.09648099 0.06146874 0.12439428 0.15775578 0.11621704\n",
      "  0.07908448 0.05244378 0.05611601 0.14904562]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.9174148415713155\n",
      "---------\n",
      "[[0.12719569 0.10455206 0.04332689 0.09707694 0.15254852 0.16264473\n",
      "  0.08384997 0.0586906  0.04380484 0.12630975]\n",
      " [0.16104253 0.08217576 0.05272239 0.09419412 0.141359   0.14731923\n",
      "  0.06235186 0.0631142  0.04554148 0.15017943]\n",
      " [0.11410354 0.12414178 0.04404879 0.10751335 0.19703421 0.12123204\n",
      "  0.0679613  0.05748994 0.05897112 0.10750393]\n",
      " [0.12507384 0.14224426 0.05583743 0.09071517 0.15947413 0.13595383\n",
      "  0.06859082 0.05466391 0.04964762 0.11779898]\n",
      " [0.11850556 0.10584106 0.05497306 0.09948641 0.15963593 0.12614808\n",
      "  0.06808863 0.04608579 0.04984605 0.17138944]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.7961356368845476\n",
      "---------\n",
      "[[0.13852235 0.11259773 0.03892751 0.07999951 0.15102795 0.18043981\n",
      "  0.07257043 0.05217341 0.0388208  0.13492048]\n",
      " [0.18197243 0.08704511 0.04705965 0.07789388 0.13875706 0.15744126\n",
      "  0.05370897 0.05546921 0.04043363 0.16021879]\n",
      " [0.12145433 0.13370696 0.03956856 0.09112477 0.20646354 0.12778073\n",
      "  0.05970433 0.05129231 0.05340841 0.11549605]\n",
      " [0.13198113 0.15821505 0.05035531 0.07650485 0.16029074 0.14394068\n",
      "  0.06037466 0.04896162 0.04437728 0.12499867]\n",
      " [0.12742568 0.11330053 0.04914636 0.08177223 0.15962123 0.13324233\n",
      "  0.05916202 0.04061094 0.04431529 0.1914034 ]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.6982123358255656\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bab21c",
   "metadata": {},
   "source": [
    "어떻습니까? 모델이 추론한 확률값 `y_hat`이 정답의 One-hot 인코딩 `t`값에 조금씩 근접하는 것과, Loss가 점점 감소하는 것이 확인되십니까?\n",
    "그렇다면 우리는 경사하강법을 통해 조금씩 파라미터를 제대로 업데이트해 가고 있다고 볼 수 있겠습니다.\n",
    "\n",
    "이제 거의 근접했습니다. 몇 가지만 보완하면 우리는 모델 훈련 전과정을 완성할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b26c1",
   "metadata": {},
   "source": [
    "# 18-7. 추론 과정 구현과 정확도(Accuracy) 계산\n",
    "\n",
    "방금 5번 학습한 파라미터 `W1`, `b1`, `W2`, `b2`를 가지고도 우리는 숫자를 인식(Predict)해 보고, 그 정확도(Accuracy)를 측정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "137f922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a93f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1476144 , 0.11913365, 0.03506685, 0.0673423 , 0.14874664,\n",
       "       0.19636118, 0.06346   , 0.04663452, 0.03454715, 0.14109332])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64209467",
   "metadata": {},
   "source": [
    "추론한 결과의 정확도가 얼마나 될지 계산해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0692ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae87077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1476144  0.11913365 0.03506685 0.0673423  0.14874664 0.19636118\n",
      " 0.06346    0.04663452 0.03454715 0.14109332]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_one_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b927ab9",
   "metadata": {},
   "source": [
    "5번의 학습만 가지고는 아직 10% 정도의 정확도에도 미치지 못하고 있습니다. 그럼 전체 학습 사이클을 제대로 수행해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35ee5a",
   "metadata": {},
   "source": [
    "# 18-8. 전체 학습 사이클 수행\n",
    "\n",
    "전체 학습 과정이 구현된 코드를 소개합니다. 아래는 학습시킬 파라미터를 초기화하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "211a3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414e52c",
   "metadata": {},
   "source": [
    "아래 코드를 통해 학습이 본격적으로 진행됩니다. GPU를 사용하지 않기 때문에 다소 시간이 걸릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ffb98be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n",
      "Loss:  2.3024695346547803\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "Loss:  0.9207486147843558\n",
      "train acc, test acc | 0.7927, 0.7987\n",
      "Loss:  0.46152335605611733\n",
      "train acc, test acc | 0.8783333333333333, 0.8829\n",
      "Loss:  0.38262757909736833\n",
      "train acc, test acc | 0.8998166666666667, 0.9027\n",
      "Loss:  0.20724683727300788\n",
      "train acc, test acc | 0.9083833333333333, 0.9111\n",
      "Loss:  0.26382261777667515\n",
      "train acc, test acc | 0.9144, 0.9166\n",
      "Loss:  0.4186355707640967\n",
      "train acc, test acc | 0.9187833333333333, 0.9198\n",
      "Loss:  0.24441667369733902\n",
      "train acc, test acc | 0.9241333333333334, 0.9255\n",
      "Loss:  0.285289214936795\n",
      "train acc, test acc | 0.9277166666666666, 0.9279\n",
      "Loss:  0.1620526769890818\n",
      "train acc, test acc | 0.9303333333333333, 0.9321\n",
      "Loss:  0.18808025100772308\n",
      "train acc, test acc | 0.9343333333333333, 0.9338\n",
      "Loss:  0.1846351317561717\n",
      "train acc, test acc | 0.9364666666666667, 0.9358\n",
      "Loss:  0.20678451509689402\n",
      "train acc, test acc | 0.9394166666666667, 0.9386\n",
      "Loss:  0.20029788085021935\n",
      "train acc, test acc | 0.94115, 0.9398\n",
      "Loss:  0.18071114934160135\n",
      "train acc, test acc | 0.9438333333333333, 0.9414\n",
      "Loss:  0.10699740511220701\n",
      "train acc, test acc | 0.9458, 0.9435\n",
      "Loss:  0.17116979459700563\n",
      "train acc, test acc | 0.94725, 0.9443\n",
      "Loss:  0.24782814661178118\n",
      "train acc, test acc | 0.9489833333333333, 0.9461\n",
      "Loss:  0.2221357431483196\n",
      "train acc, test acc | 0.95055, 0.9466\n",
      "Loss:  0.17654868188654363\n",
      "train acc, test acc | 0.9517333333333333, 0.9484\n",
      "Loss:  0.11824684851913156\n",
      "train acc, test acc | 0.9529333333333333, 0.9493\n",
      "Loss:  0.20998327687558466\n",
      "train acc, test acc | 0.9538833333333333, 0.9498\n",
      "Loss:  0.0951359530483626\n",
      "train acc, test acc | 0.9554833333333334, 0.9518\n",
      "Loss:  0.1473039683703209\n",
      "train acc, test acc | 0.95645, 0.9533\n",
      "Loss:  0.08335056662721253\n",
      "train acc, test acc | 0.9576333333333333, 0.9532\n",
      "Loss:  0.10939415145221142\n",
      "train acc, test acc | 0.9586166666666667, 0.9535\n",
      "Loss:  0.14484673777209373\n",
      "train acc, test acc | 0.9591666666666666, 0.9552\n",
      "Loss:  0.22056585367792633\n",
      "train acc, test acc | 0.96045, 0.9552\n",
      "Loss:  0.1652011680660989\n",
      "train acc, test acc | 0.9611666666666666, 0.9565\n",
      "Loss:  0.16755013425775772\n",
      "train acc, test acc | 0.96155, 0.9564\n",
      "Loss:  0.09969468687507446\n",
      "train acc, test acc | 0.96295, 0.9571\n",
      "Loss:  0.11773139718153758\n",
      "train acc, test acc | 0.9636166666666667, 0.9567\n",
      "Loss:  0.10929147932333903\n",
      "train acc, test acc | 0.9644333333333334, 0.9586\n",
      "Loss:  0.17192951996607178\n",
      "train acc, test acc | 0.9652333333333334, 0.9598\n",
      "Loss:  0.11313330959520947\n",
      "train acc, test acc | 0.966, 0.96\n",
      "Loss:  0.1054003001317038\n",
      "train acc, test acc | 0.9668, 0.9614\n",
      "Loss:  0.12198627091726616\n",
      "train acc, test acc | 0.9673666666666667, 0.9613\n",
      "Loss:  0.13682199875487494\n",
      "train acc, test acc | 0.9677, 0.9609\n",
      "Loss:  0.10880559871314167\n",
      "train acc, test acc | 0.96835, 0.9608\n",
      "Loss:  0.15935745027202858\n",
      "train acc, test acc | 0.9693666666666667, 0.9623\n",
      "Loss:  0.09094733392179752\n",
      "train acc, test acc | 0.9692333333333333, 0.9623\n",
      "Loss:  0.1513732602469328\n",
      "train acc, test acc | 0.97015, 0.9627\n",
      "Loss:  0.10100082543287645\n",
      "train acc, test acc | 0.9707833333333333, 0.9622\n",
      "Loss:  0.12866842993601982\n",
      "train acc, test acc | 0.9712666666666666, 0.9623\n",
      "Loss:  0.09220434818052899\n",
      "train acc, test acc | 0.9714833333333334, 0.9627\n",
      "Loss:  0.07894083803114174\n",
      "train acc, test acc | 0.9722333333333333, 0.9627\n",
      "Loss:  0.07880283989384998\n",
      "train acc, test acc | 0.97245, 0.9635\n",
      "Loss:  0.129384835210459\n",
      "train acc, test acc | 0.9731333333333333, 0.9644\n",
      "Loss:  0.05784563588460684\n",
      "train acc, test acc | 0.97355, 0.9637\n",
      "Loss:  0.1140828088985783\n",
      "train acc, test acc | 0.97355, 0.9652\n",
      "Loss:  0.08016153208069954\n",
      "train acc, test acc | 0.9740833333333333, 0.964\n",
      "Loss:  0.06219955999768585\n",
      "train acc, test acc | 0.97435, 0.965\n",
      "Loss:  0.11624179640526418\n",
      "train acc, test acc | 0.9747666666666667, 0.9651\n",
      "Loss:  0.06982430612461815\n",
      "train acc, test acc | 0.97485, 0.9652\n",
      "Loss:  0.04261314905929527\n",
      "train acc, test acc | 0.9751166666666666, 0.9656\n",
      "Loss:  0.1829658368500044\n",
      "train acc, test acc | 0.9758166666666667, 0.9661\n",
      "Loss:  0.12905544618560144\n",
      "train acc, test acc | 0.9756166666666667, 0.9661\n",
      "Loss:  0.17846445227125396\n",
      "train acc, test acc | 0.9762666666666666, 0.9663\n",
      "Loss:  0.1653143626179703\n",
      "train acc, test acc | 0.9767166666666667, 0.967\n",
      "Loss:  0.06072956714814239\n",
      "train acc, test acc | 0.97685, 0.9677\n",
      "Loss:  0.08177898318747134\n",
      "train acc, test acc | 0.9772, 0.9671\n",
      "Loss:  0.037528046121492194\n",
      "train acc, test acc | 0.9775333333333334, 0.9677\n",
      "Loss:  0.057632546424616\n",
      "train acc, test acc | 0.97745, 0.9682\n",
      "Loss:  0.09758263769784489\n",
      "train acc, test acc | 0.9778666666666667, 0.9679\n",
      "Loss:  0.06676552942544715\n",
      "train acc, test acc | 0.97865, 0.9675\n",
      "Loss:  0.08201091916380861\n",
      "train acc, test acc | 0.9788666666666667, 0.9679\n",
      "Loss:  0.06961853859789456\n",
      "train acc, test acc | 0.9792833333333333, 0.968\n",
      "Loss:  0.07022478697650932\n",
      "train acc, test acc | 0.9795333333333334, 0.9678\n",
      "Loss:  0.06379570893185914\n",
      "train acc, test acc | 0.9794666666666667, 0.9677\n",
      "Loss:  0.07795533561943724\n",
      "train acc, test acc | 0.9795166666666667, 0.9684\n",
      "Loss:  0.03573297314056833\n",
      "train acc, test acc | 0.98035, 0.9685\n",
      "Loss:  0.13381170687066493\n",
      "train acc, test acc | 0.9803, 0.9698\n",
      "Loss:  0.03618664494664914\n",
      "train acc, test acc | 0.9804666666666667, 0.9692\n",
      "Loss:  0.08729692416352322\n",
      "train acc, test acc | 0.9806, 0.9688\n",
      "Loss:  0.039109061697071366\n",
      "train acc, test acc | 0.98095, 0.9698\n",
      "Loss:  0.05714515305264506\n",
      "train acc, test acc | 0.9815833333333334, 0.9695\n",
      "Loss:  0.06602822541745224\n",
      "train acc, test acc | 0.98155, 0.9693\n",
      "Loss:  0.052103220983321645\n",
      "train acc, test acc | 0.9816166666666667, 0.9699\n",
      "Loss:  0.07903615439033321\n",
      "train acc, test acc | 0.9816666666666667, 0.9696\n",
      "Loss:  0.03935567521115586\n",
      "train acc, test acc | 0.9818833333333333, 0.9698\n",
      "Loss:  0.04490861352262301\n",
      "train acc, test acc | 0.9821333333333333, 0.9698\n",
      "Loss:  0.03535369300358493\n",
      "train acc, test acc | 0.9823166666666666, 0.9698\n",
      "Loss:  0.029018105973210247\n",
      "train acc, test acc | 0.9826, 0.9694\n",
      "Loss:  0.14108976681088012\n",
      "train acc, test acc | 0.9826333333333334, 0.97\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4ad2f",
   "metadata": {},
   "source": [
    "어떻습니까? 딥러닝 프레임워크가 없이도 Numpy만으로도 딥러닝이 가능하다는 것을 확인했습니다. 물론 아쉽게도 Numpy는 GPU를 지원하지 않으므로, 방금의 딥러닝은 CPU만 사용한 것입니다. 하지만 이로써 우리는 딥러닝 프레임워크 안에서 일어나는 일의 대강에 대해서 추론해 볼 수 있게 되었습니다.\n",
    "\n",
    "마지막으로 위 훈련 과정의 Accuracy, Loss 변화를 시각화해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5ce17df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c68d202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a0fd1",
   "metadata": {},
   "source": [
    "지금까지 그동안 막연히 개념적으로만 이해해 왔던, 텐서플로우 같은 딥러닝 프레임워크에서 자동으로 수행되기 때문에 구체적인 동작 메커니즘이 잘 와닿지 않았던 딥러닝이라는 것을 좀 더 세부적으로 들여다보았습니다. 경사하강법과 오차역전파법을 통해 파라미터가 어떻게 최적화되는지를 이해하는데 오늘의 내용이 도움이 되었기를 바랍니다. 수고하셨습니다!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
